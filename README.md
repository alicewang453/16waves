## Intro
Online quizzes that tell users something about themselves are a popular way for users to engage lightheartedly and enjoyably in a topic. Standard personality quizzes such as the 16personalities test rely on questions about the user’s own life and experiences to give the user a result (in this case an MBTI personality type). However, websites such as Buzzfeed have popularized quizzes that rely on atypical measures to assign a personality trait. Inspired by the success of this format, we decided to make a quiz that assigns the user an MBTI personality type based on hearing and choosing their favorite samples made using computational sound techniques. This would present a topic that is often confusing and esoteric to the general public (digital audio) in a popular and digestible format.
Our aim was to create an interactive personality test using digitally generated sound preferences that users with little knowledge of computational sound, digital music production, or physical modeling could have fun doing while learning about these topics in the process.

The core structure of this project is a webpage in the React.js framework that uses the Web Audio API to generate sounds. Notable files in our repository are constants.js and Quiz.jsx.


## Design
constants.js is a collection of javascript objects. These include the questions and answer choices of our quiz, the initial state of an object that tracks the user’s personality metrics as they go through the quiz, and a collection of possible descriptions for the user’s personality result.

Quiz.jsx is the main quiz component of the webpage. Its most important sections are:
- The state variables used across all the questions of the quiz. These include variables such as audioContext, globalGain, and analyser (graph of waves) that are set at the beginning and stay constant, as well as variables that change frequently such as the source of the current sound (oscillator nodes, buffer source), and the values associated with the user’s selected answer. The state variable result is an Object that updates with parameters to calculate the user’s personality as they answer each question.
- The page source that is returned at the end. The most notable part of this is the set of answer choices, which upon selecting or sliding and playing, call a function to play the sound corresponding to the question and answer choice. 
- The functions that play a certain sound. Because of the different ways in which noise buffers and oscillators are generated in Web Audio, there are multiple different functions, and the specific function called depends on the current question’s type (e.g., does the question play noise, play an oscillator, play physically modeled sound, etc). Notable functions are:
  - generateNoise, which creates a buffer and loads different values into it depending on the color of noise selected by the user. Pink, brown, and blue noise are generated simply using the white noise base code with calculations added to filter out the highs in the case of pink and brown, or to emphasize them and deemphasize the lows for blue noise. This is then fed into playNoise, which connects the buffer into a buffer source and then the source to a gain and starts it. playNoise also updates the relevant state variables (source, whether it is currently playing, and the identity of the current sound).
  - playSound, which creates a buffer source for the desired physically modeled sound, models the sound using noise and filters, and plays it and updates relevant state variables. For example, the babbling brook sound was generated by two buffers of brown noise (using the generateNoise function), which were connected to two low pass filters and then a resonant high pass filter and the frequency of the filter respectively to give the filter a sliding randomness, simulating the swelling and diminishing sound of a brook.
  - generateOscs, which creates the needed oscillators for musical synthesis questions. For example, for the question about four different waveforms, it generates an oscillator of that waveform for each answer choice, decreasing the gain on saw and square waves. These are then connected to the global gain and started, and relevant variables updated. Similar methods are used for additive synthesis, AM/FM, etc.
- The draw function for the analyser/visualizer of the sound being played. This is a canvas where lines are drawn to match the characteristics of the sound. For oscillator sounds, simply the global gain is connected to the analyser so that it can show the oscillators. For sounds that begin with noise, we connect the noise buffer source to the analyser and then call the draw function again to make the graph show lines that correspond to the buffer source.
- onClickNext, which activates when the next button is clicked to go to the next question. It stops any sound being played, updates relevant state variables, and then updates the result state variable with the new changes to personality parameters gained from the user’s answer. 
- useEffect, a React hook here used mainly to initialize the audioContext, globalGain, and analyser at the start of loading the webpage.

## Conclusions
In this project, we designed and implemented a quiz using the React.js framework. Through the structure of a casual personality test, we introduce computational sound concepts to users that may not have any background knowledge.

Because of time constraints, current functionality is limited to fairly basic computational sound concepts. We have these plans for future improvements: To pivot further into the educational aim, questions could have more in-depth explanations of the sounds being generated, and they could be reordered to make sense for building on concepts sequentially. To increase the uniqueness and interactivity of the quiz, we could add questions with more unique sounds and more interesting computational sound concepts. For example, a fairly ambitious idea is teaching the user to live code briefly, and analyzing their live coding results to contribute to personality information. 
